# Final project - CL course 
The complete project report is available in [pdf format](https://github.com/riccardoc95/CL_course-final_project/blob/main/overleaf/Progetto_CL.pdf).

## Intro
This project aims to develop a generative replay method for continual learning using Generative Adversarial Networks (GANs) as the generative model. The goal is to evaluate this method in a class-incremental learning scenario, where the MNIST dataset is divided into five tasks, each containing two classes. This approach addresses the challenge of catastrophic forgetting by generating replay samples for previously learned tasks, allowing the model to retain knowledge over time.

In a class-incremental scenario, the model sequentially learns new classes without revisiting old data. The generative replay method leverages GANs to produce samples from past tasks, which are then used during training on new tasks to maintain performance on earlier tasks. The effectiveness of this method will be assessed using three metrics: average accuracy, forward transfer (FWT), and backward transfer (BWT). 

## Results
In the vanilla method, all images from the previous task are stored and used for training the next task. Conversely, the GAN method involves training a single GAN network on the previous tasks to generate samples for predicting the total learning loss on those tasks.

The hyperparameters for training the classifier are the same for both methods: a learning rate of $1e^{-5}$, a batch size of $128$, and $50$ epochs on a $3$-layer MLP with a hidden layer size of $256$. A small learning rate is set to prevent the loss of previously acquired information. For the GAN, $1000$ samples are generated from the previous tasks.

The results indicate that the GAN-based method yields lower accuracy. This lower performance may be due to several factors, primarily that the generated samples from previous tasks are fewer than the actual dataset in the vanilla method. Additionally, the generated dataset might be imbalanced, leading to a decrease in accuracy for certain classes, such as class 1, with each iteration.

### Vanilla
![image](https://github.com/riccardoc95/CL_course-final_project/blob/main/overleaf/imgs/vanilla1.png)
![image](https://github.com/riccardoc95/CL_course-final_project/blob/main/overleaf/imgs/vanilla2.png)

### GAN
![image](https://github.com/riccardoc95/CL_course-final_project/blob/main/overleaf/imgs/gan1.png)
![image](https://github.com/riccardoc95/CL_course-final_project/blob/main/overleaf/imgs/gan2.png)

## Improving
To improve the accuracy of the presented architecture, several enhancements can be made. Increasing the number of samples generated by the GAN for previous tasks is one approach. Additionally, upgrading the classifier to a convolutional network could enhance performance. To address dataset imbalance, a separate GAN can be trained for each previous task, allowing multiple GAN generators to create samples from those tasks.

Furthermore, memory efficiency can be improved by generating GAN samples in batches rather than all at once. This method enables processing each batch with the classifier sequentially, thereby allowing a large number of samples to be generated without significantly impacting memory usage.

